@inbook{colomboni2025fatshattering,
    author = {Roberto Colomboni and Emmanuel Esposito and Andrea Paudice},
    title = {An Improved Uniform Convergence Bound with Fat-Shattering Dimension},
    booktitle = {Information Processing Letters},
    chapter = {},
    pages = {},
    year = {2025},
    volume = {188},
    publisher = {Elsevier},
    doi = {10.1016/j.ipl.2024.106539},
    url = {https://www.sciencedirect.com/science/article/abs/pii/S0020019024000693},
    eprint = {https://arxiv.org/pdf/2307.06644},
    html = {https://www.sciencedirect.com/science/article/abs/pii/S0020019024000693},
    arxiv = {2307.06644},
    abbr = {IPL},
    abstract = {The fat-shattering dimension characterizes the uniform convergence property of real-valued functions. The state-of-the-art upper bounds feature a multiplicative squared logarithmic factor on the sample complexity, leaving an open gap with the existing lower bound. We provide an improved uniform convergence bound that closes this gap.}
}

@article{bressan2024interpretable,
    author = {Marco Bressan and Nicol{\`{o}} Cesa{-}Bianchi and Emmanuel Esposito and Yishay Mansour and Shay Moran and Maximilian Thiessen},
    title = {A Theory of Interpretable Approximations},
    year = {2024},
    booktitle = {COLT '24},
    journal = {COLT '24},
    arxiv = {2406.10529},
    abbr = {COLT},
    selected = {true},
    abstract = {Can a deep neural network be approximated by a small decision tree based on simple features? This question and its variants are behind the growing demand for machine learning models that are \emph{interpretable} by humans. In this work we study such questions by introducing \emph{interpretable approximations}, a notion that captures the idea of approximating a target concept $c$ by a small aggregation of concepts from some base class $\mathcal{H}$. In particular, we consider the approximation of a binary concept $c$ by decision trees based on a simple class $\mathcal{H}$ (e.g., of bounded VC dimension), and use the tree depth as a measure of complexity. Our primary contribution is the following remarkable trichotomy. For any given pair of $\mathcal{H}$ and $c$, exactly one of these cases holds: (i) $c$ cannot be approximated by $\mathcal{H}$ with arbitrary accuracy; (ii) $c$ can be approximated by $\mathcal{H}$ with arbitrary accuracy, but there exists no universal rate that bounds the complexity of the approximations as a function of the accuracy; or (iii) there exists a constant $\kappa$ that depends only on $\mathcal{H}$ and $c$ such that, for \emph{any} data distribution and \emph{any} desired accuracy level, $c$ can be approximated by $\mathcal{H}$ with a complexity not exceeding $\kappa$. This taxonomy stands in stark contrast to the landscape of supervised classification, which offers a complex array of distribution-free and universally learnable scenarios. We show that, in the case of interpretable approximations, even a slightly nontrivial a-priori guarantee on the complexity of approximations implies approximations with constant (distribution-free and accuracy-free) complexity. We extend our trichotomy to classes $\mathcal{H}$ of unbounded VC dimension and give characterizations of interpretability based on the algebra generated by $\mathcal{H}$.}
}

@article{bressan2024monophonic,
    author = {Marco Bressan and Emmanuel Esposito and Maximilian Thiessen},
    title = {Efficient Algorithms for Learning Monophonic Halfspaces in Graphs},
    year = {2024},
    booktitle = {COLT '24},
    journal = {COLT '24},
    arxiv = {2405.00853},
    abbr = {COLT},
    selected = {true},
    abstract = {We study the problem of learning a binary classifier on the vertices of a graph. In particular, we consider classifiers given by monophonic halfspaces, partitions of the vertices that are convex in a certain abstract sense. Monophonic halfspaces, and related notions such as geodesic halfspaces, have recently attracted interest, and several connections have been drawn between their properties (e.g., their VC dimension) and the structure of the underlying graph $G$. We prove several novel results for learning monophonic halfspaces in the supervised, online, and active settings. Our main result is that a monophonic halfspace can be learned with near-optimal passive sample complexity in time polynomial in $n = |V(G)|$. This requires us to devise a polynomial-time algorithm for consistent hypothesis checking, based on several structural insights on monophonic halfspaces and on a reduction to 2-satisfiability. We prove similar results for the online and active settings. We also show that the concept class can be enumerated with delay $\mathrm{poly}(n)$, and that empirical risk minimization can be performed in time $2\omega(G) \mathrm{poly}(n)$ where $\omega(G)$ is the clique number of $G$. These results answer open questions from the literature (González et al., 2020), and show a contrast with geodesic halfspaces, for which some of the said problems are NP-hard (Seiffarth et al., 2023).}
}

@article{eldowa2023feedbackgraphs,
    author = {Khaled Eldowa and Emmanuel Esposito and Tommaso Cesari and Nicol{\`{o}} Cesa{-}Bianchi},
    title = {On the Minimax Regret for Online Learning with Feedback Graphs},
    year = {2023},
    booktitle = {NeurIPS '23 (Spotlight)},
    journal = {NeurIPS '23},
    arxiv = {2305.15383},
    abbr = {NeurIPS},
    selected = {true},
    toappear = {true},
    abstract = {In this work, we improve on the upper and lower bounds for the regret of online learning with strongly observable undirected feedback graphs. The best known upper bound for this problem is $\mathcal{O}\bigl(\sqrt{\alpha T\ln K}\bigr)$, where $K$ is the number of actions, $\alpha$ is the independence number of the graph, and $T$ is the time horizon. The $\sqrt{\ln K}$ factor is known to be necessary when $\alpha = 1$ (the experts case). On the other hand, when $\alpha = K$ (the bandits case), the minimax rate is known to be $\Theta\bigl(\sqrt{KT}\bigr)$, and a lower bound $\Omega\bigl(\sqrt{\alpha T}\bigr)$ is known to hold for any $\alpha$. Our improved upper bound $\mathcal{O}\bigl(\sqrt{\alpha T(1+\ln(K/\alpha))}\bigr)$ holds for any $\alpha$ and matches the lower bounds for bandits and experts, while interpolating intermediate cases. To prove this result, we use FTRL with $q$-Tsallis entropy for a carefully chosen value of $q \in [1/2, 1)$ that varies with $\alpha$. The analysis of this algorithm requires a new bound on the variance term in the regret. We also show how to extend our techniques to time-varying graphs, without requiring prior knowledge of their independence numbers. Our upper bound is complemented by an improved $\Omega\bigl(\sqrt{\alpha T(\ln K)/(\ln\alpha)}\bigr)$ lower bound for all $\alpha > 1$, whose analysis relies on a novel reduction to multitask learning. This shows that a logarithmic factor is necessary as soon as $\alpha < K$.}
}

@article{esposito2023intermediateobservations,
    author = {Emmanuel Esposito and Saeed Masoudian and Hao Qiu and Dirk {van\ der\ Hoeven} and Nicol{\`{o}} Cesa{-}Bianchi and Yevgeny Seldin},
    title = {Delayed Bandits: When Do Intermediate Observations Help?},
    year = {2023},
    booktitle = {ICML '23},
    journal = {ICML '23},
    url = {https://proceedings.mlr.press/v202/esposito23a.html},
    html = {https://proceedings.mlr.press/v202/esposito23a.html},
    arxiv = {2305.19036},
    abbr = {ICML},
    selected = {true},
    abstract = {We study a $K$-armed bandit with delayed feedback and intermediate observations. We consider a model where intermediate observations have a form of a finite state, which is observed immediately after taking an action, whereas the loss is observed after an adversarially chosen delay. We show that the regime of the mapping of states to losses determines the complexity of the problem, irrespective of whether the mapping of actions to states is stochastic or adversarial. If the mapping of states to losses is adversarial, then the regret rate is of order $\sqrt{(K+d)T}$ (within log factors), where $T$ is the time horizon and $d$ is a fixed delay. This matches the regret rate of a $K$-armed bandit with delayed feedback and without intermediate observations, implying that intermediate observations are not helpful. However, if the mapping of states to losses is stochastic, we show that the regret grows at a rate of $\sqrt{\bigl(K+\min\{|\mathcal{S}|,d\}\bigr)T}$ (within log factors), implying that if the number $|\mathcal{S}|$ of states is smaller than the delay, then intermediate observations help. We also provide refined high-probability regret upper bounds for non-uniform delays, together with experimental validation of our algorithms.}
}

@article{esposito2022stochasticfeedback,
    author = {Emmanuel Esposito and Federico Fusco and Dirk {van\ der\ Hoeven} and Nicol{\`{o}} Cesa{-}Bianchi},
    title = {Learning on the Edge: Online Learning with Stochastic Feedback Graphs},
    year = {2022},
    booktitle = {NeurIPS '22},
    journal = {NeurIPS '22},
    url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/e0e956681b04ac126679e8c7dd706b2e-Abstract-Conference.html},
    html = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/e0e956681b04ac126679e8c7dd706b2e-Abstract-Conference.html},
    arxiv = {2210.04229},
    abbr = {NeurIPS},
    selected = {true},
    abstract = {The framework of feedback graphs is a generalization of sequential decision-making with bandit or full information feedback. In this work, we study an extension where the directed feedback graph is stochastic, following a distribution similar to the classical Erdős-Rényi model. Specifically, in each round every edge in the graph is either realized or not with a distinct probability for each edge. We prove nearly optimal regret bounds of order $\min\bigl\{\min_{\varepsilon} \sqrt{(\alpha_\varepsilon/\varepsilon) T},\, \min_{\varepsilon} (\delta_\varepsilon/\varepsilon)^{1/3} T^{2/3}\bigr\}$ (ignoring logarithmic factors), where $\alpha_{\varepsilon}$ and $\delta_{\varepsilon}$ are graph-theoretic quantities measured on the support of the stochastic feedback graph $\mathcal{G}$ with edge probabilities thresholded at~$\varepsilon$. Our result, which holds without any preliminary knowledge about $\mathcal{G}$, requires the learner to observe only the realized out-neighborhood of the chosen action. When the learner is allowed to observe the realization of the entire graph (but only the losses in the out-neighborhood of the chosen action), we derive a more efficient algorithm featuring a dependence on weighted versions of the independence and weak domination numbers that exhibits improved bounds for some special cases.}
}

@inbook{esposito2020recsplit,
    author = {Emmanuel Esposito and Thomas Mueller Graf and Sebastiano Vigna},
    title = {{RecSplit}: Minimal Perfect Hashing via Recursive Splitting},
    booktitle = {2020 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX)},
    chapter = {},
    pages = {175--185},
    year = {2020},
    publisher = {SIAM},
    doi = {10.1137/1.9781611976007.14},
    url = {https://epubs.siam.org/doi/abs/10.1137/1.9781611976007.14},
    eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611976007.14},
    html = {https://epubs.siam.org/doi/abs/10.1137/1.9781611976007.14},
    arxiv = {1910.06416},
    code = {https://github.com/vigna/sux},
    abbr = {ALENEX},
    bibtex_show = {true},
    abstract = {A minimal perfect hash function bijectively maps a key set $S$ out of a universe $U$ into the first $|S|$ natural numbers. Minimal perfect hash functions are used, for example, to map irregularly-shaped keys, such as strings, in a compact space so that metadata can then be simply stored in an array. While it is known that just 1.44 bits per key are necessary to store a minimal perfect hash function, no published technique can go below 2 bits per key in practice. We propose a new technique for storing minimal perfect hash functions with expected linear construction time and expected constant lookup time that makes it possible to build for the first time, for example, structures which need 1.56 bits per key, that is, within 8.3\% of the lower bound, in less than 2 ms per key. We show that instances of our construction are able to simultaneously beat the construction time, space usage and lookup time of the state-of-the-art data structure reaching 2 bits per key. Moreover, we provide parameter choices giving structures which are competitive with alternative, larger-size data structures in terms of space and lookup time. The construction of our data structures can be easily parallelized or mapped on distributed computational units (e.g., within the MapReduce framework), and structures larger than the available RAM can be directly built in mass storage.}
}
